Metadata-Version: 2.4
Name: circuitscaling
Version: 0.1.0
Summary: Scaling behavior of transformer circuits (copy-suppression, SVA)
Author: Tejas Dahiya
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Dynamic: license-file

# Circuit-Scaling: How Transformer Circuits Evolve with Model Size

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

**Author:** Tejas Dahiya (University of Wisconsinâ€“Madison)  
**Status:** Research in progress â€” targeting submission to NeurIPS / ICLR / ICML

> A cross-size, cross-family causal analysis showing how functional transformer circuits (IOI, Anti-Repeat, Copy-Suppression) strengthen, migrate, and organize as model size increases from 70M â†’ 1B+ parameters.

---

## Overview

Most interpretability research analyzes only a single model size (e.g., GPT-2 Small). This project answers a deeper question:

> **Do functional transformer circuits persist, strengthen, or reorganize as model size increases?**

We study scaling behavior across **GPT-2**, **Pythia**, **GPT-Neo**, and **OPT** families, focusing on:

- **IOI (Induction-like) heads** â€” Indirect Object Identification circuits
- **Anti-Repeat / Copy-Suppression heads** â€” Repetition inhibition mechanisms
- **Shared directional heads** â€” Dual-function attention patterns
- **Layer migration across sizes** â€” How circuits relocate with scale
- **Causal functional validation** â€” Directional influence verification
- **Novel ablation tests** â€” Hero-Heads vs Random Heads comparison

All experiments conducted using [TransformerLens](https://github.com/neelnanda-io/TransformerLens).

---

## Key Findings

| Finding | Description |
|---------|-------------|
| ðŸ”¬ **Cross-family scaling laws** | IOI â†” Anti-Repeat correlations evolve predictably with parameter count |
| ðŸ§  **Dual-function heads exist** | A subset of heads perform both IOI and anti-repeat tasks |
| ðŸ“ˆ **Strong heads get stronger** | Head magnitude scales with model size |
| ðŸŽ¯ **Hero-head ablation** | Functional heads show significantly higher ablation impact than random baselines |

---

## Repository Structure

```
circuit-scaling/
â”œâ”€â”€ paper/
â”‚   â”œâ”€â”€ figs/           # All figures (PNG/PDF)
â”‚   â”œâ”€â”€ tables/         # CSV results from experiments
â”‚   â””â”€â”€ notes/          # High-level findings & paper outline
â”œâ”€â”€ scripts/            # Experiment scripts
â”œâ”€â”€ src/                # Core analysis modules
â”œâ”€â”€ results/            # Raw pipeline outputs
â”œâ”€â”€ environment.yml     # Conda environment
â””â”€â”€ README.md
```

---

## Installation

```bash
git clone https://github.com/Tejas7007/circuit-scaling.git
cd circuit-scaling

conda env create -f environment.yml
conda activate circuit-scaling
```

---

## Reproducing All Experiments

### 1. IOI & Anti-Repeat Strength Scaling
```bash
python scripts/run_joint_head_scan.py
```

### 2. Causal Tracing
```bash
python scripts/run_causal_tracing.py
```

### 3. Hero-Head Ablation (Novel)
```bash
python scripts/ablate_hero_heads_vs_random.py
python scripts/summarize_ablation_results.py
```

### 4. Generate Tables & Figures
```bash
python scripts/summarize_head_categories.py
python scripts/summarize_scaling_correlations.py
```

---

## Figures

All figures stored in `paper/figs/`.

### 1. Cross-Family Correlation vs Model Size

Shows how IOI â†” Anti-Repeat correlations evolve across GPT-2, GPT-Neo, Pythia, and OPT.

![Cross-Family Correlation](./paper/figs/corr_vs_scale_cross_family.png)

### 2. Strong Head Scaling Law

Demonstrates that strong heads become stronger, sharper, and more numerous with scale.

![Strong Head Scaling](./paper/figs/strong_head_scaling.png)

### 3. Phase-Space Plot Across All Heads (720+)

A unified 2D projection of all heads: IOI-dominant, Anti-Repeat dominant, Shared, and Weak.

![Phase Space](./paper/figs/all_heads_phase_space.png)

### 4. Family-Grid Visualization

Each subplot contains hundreds of heads mapped into Î”IOI vs Î”Anti-repeat space.

![Family Grid](./paper/figs/family_grid.png)

### 5. Per-Model Layer Histograms

| GPT-Neo-125M | GPT-2 |
|:------------:|:-----:|
| ![GPT-Neo-125M](./paper/figs/layer_hist_gpt-neo_gpt-neo-125M.png) | ![GPT-2](./paper/figs/layer_hist_gpt2_gpt2.png) |

| GPT-2 Medium | GPT-2 Large |
|:------------:|:-----------:|
| ![GPT-2 Medium](./paper/figs/layer_hist_gpt2_gpt2-medium.png) | ![GPT-2 Large](./paper/figs/layer_hist_gpt2_gpt2-large_tau003.png) |

| Pythia-70M | Pythia-160M | Pythia-410M | Pythia-1B |
|:----------:|:-----------:|:-----------:|:---------:|
| ![Pythia-70M](./paper/figs/layer_hist_pythia_pythia-70m.png) | ![Pythia-160M](./paper/figs/layer_hist_pythia_pythia-160m.png) | ![Pythia-410M](./paper/figs/layer_hist_pythia_pythia-410m.png) | ![Pythia-1B](./paper/figs/layer_hist_pythia_pythia-1b.png) |

| OPT-125M |
|:--------:|
| ![OPT-125M](./paper/figs/layer_hist_opt_opt-125m.png) |

### 6. Î”IOI vs Î”Anti Scatter Plots

| GPT-2 | GPT-2 Medium | GPT-2 Large |
|:-----:|:------------:|:-----------:|
| ![GPT-2](./paper/figs/gpt2_gpt2_delta_scatter.png) | ![GPT-2 Medium](./paper/figs/gpt2_gpt2-medium_delta_scatter.png) | ![GPT-2 Large](./paper/figs/gpt2_gpt2-large_delta_scatter.png) |

| Pythia-70M | Pythia-160M | Pythia-410M | Pythia-1B |
|:----------:|:-----------:|:-----------:|:---------:|
| ![Pythia-70M](./paper/figs/pythia_pythia-70m_delta_scatter.png) | ![Pythia-160M](./paper/figs/pythia_pythia-160m_delta_scatter.png) | ![Pythia-410M](./paper/figs/pythia_pythia-410m_delta_scatter.png) | ![Pythia-1B](./paper/figs/pythia_pythia-1b_delta_scatter.png) |

| GPT-Neo-125M | OPT-125M |
|:------------:|:--------:|
| ![GPT-Neo-125M](./paper/figs/gpt-neo_gpt-neo-125M_delta_scatter.png) | ![OPT-125M](./paper/figs/opt_opt-125m_delta_scatter.png) |

### 7. Causal Tracing Validation

Causally validates directional influence of strong heads.

| IOI Causal Tracing | Anti-Repeat Causal Tracing |
|:------------------:|:--------------------------:|
| ![IOI Causal](./paper/figs/hero_head_causal_scatter_ioi.png) | ![Anti Causal](./paper/figs/hero_head_causal_scatter_anti.png) |

### 8. Hero-Head vs Random Ablation (Novel Experiment)

Ablating identified "hero heads" causes a predictable performance drop, unlike random heads â€” strong evidence of functional circuits.

![Hero vs Random Ablation](./paper/figs/gpt2_gpt2-large_hero_vs_random_ablation.png)

---

## Data Tables

```
paper/tables/
â”œâ”€â”€ corr_vs_scale_cross_family.csv      # Correlation scaling data
â”œâ”€â”€ head_category_summary.csv           # Head type distributions
â”œâ”€â”€ hero_head_causal_summary.csv        # Causal tracing + head strengths
â”œâ”€â”€ hero_heads_for_paper.csv            # Top functional heads classified
â”œâ”€â”€ hero_vs_random_ablation_summary.csv # Ablation experiment results
â””â”€â”€ joint_ioi_anti_repeat_heads.csv     # Unified Î”IOI and Î”Anti metrics
```

---

## Models Studied

| Family | Models | Parameters |
|--------|--------|------------|
| **GPT-2** | gpt2, gpt2-medium, gpt2-large, gpt2-xl | 124M â€“ 1.5B |
| **Pythia** | pythia-70m, pythia-160m, pythia-410m, pythia-1b, pythia-1.4b, pythia-2.8b | 70M â€“ 2.8B |
| **OPT** | opt-125m, opt-350m, opt-1.3b, opt-2.7b | 125M â€“ 2.7B |
| **GPT-Neo** | gpt-neo-125m, gpt-neo-1.3b, gpt-neo-2.7b | 125M â€“ 2.7B |

---

## Paper Outline

### 1. Introduction
- Why scaling analysis matters for interpretability
- Gap in existing literature
- Contributions

### 2. Methods
- Head influence metrics (Î”IOI, Î”Anti construction)
- Causal tracing framework
- Ablation methodology
- Cross-family comparison approach

### 3. Results
- Scaling law for head strength
- Migration of IOI/Anti heads across layers
- Cross-family conservation patterns
- Phase-space clustering
- Causal directional influence
- Novel ablation: Hero-Heads vs Random

### 4. Discussion
- Circuit specialization with scale
- Emergence of strong inhibitory heads
- Implications for mechanistic interpretability

### 5. Limitations

### 6. Future Work
- Extension to modern models (Llama-3.x, Gemma)
- SAE-aligned feature scaling
- Induction/copy-suppression interaction analysis

### 7. Appendix
- All figures, tables, prompt templates, hyperparameters

---

## Citation

```bibtex
@misc{dahiya2025circuitscaling,
  author       = {Dahiya, Tejas},
  title        = {Circuit-Scaling: How Transformer Circuits Evolve with Model Size},
  year         = {2025},
  publisher    = {GitHub},
  howpublished = {\url{https://github.com/Tejas7007/circuit-scaling}}
}
```

---

## Acknowledgements

- [TransformerLens](https://github.com/neelnanda-io/TransformerLens) (Nanda et al.)
- [HuggingFace Transformers](https://huggingface.co/) for model weights

---

## License

MIT License â€” see [LICENSE](LICENSE) for details.
